"""
TDK SÃ¶zlÃ¼k veri setini yÃ¼kleyen ve iÅŸleyen modÃ¼l.

Bu modÃ¼l Hugging Face'den veri setini indirir, temizler ve
RAG sistemi iÃ§in uygun formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
"""

from datasets import load_dataset
import pandas as pd
from tqdm import tqdm
import json
import os


class TDKDataLoader:
    """TDK SÃ¶zlÃ¼k veri setini yÃ¼kler ve iÅŸler."""

    def __init__(self, cache_dir="./data"):
        """
        Args:
            cache_dir: Veri setinin kaydedileceÄŸi klasÃ¶r
        """
        self.cache_dir = cache_dir
        self.dataset = None
        self.processed_data = []

        # KlasÃ¶r yoksa oluÅŸtur
        os.makedirs(cache_dir, exist_ok=True)

    def load_dataset(self):
        """
        Hugging Face'den TDK SÃ¶zlÃ¼k veri setini yÃ¼kler.

        Returns:
            Dataset objesi
        """
        print("ğŸ“š TDK SÃ¶zlÃ¼k veri seti yÃ¼kleniyor...")

        try:
            # Veri setini yÃ¼kle
            self.dataset = load_dataset("Ba2han/TDK_Sozluk-Turkish-v2")
            print(f"Veri seti baÅŸarÄ±yla yÃ¼klendi!")
            print(f"Toplam kayÄ±t sayÄ±sÄ±: {len(self.dataset['train'])}")

            return self.dataset

        except Exception as e:
            print(f"Veri seti yÃ¼klenirken hata: {e}")
            return None

    def explore_data(self):
        """Veri setinin yapÄ±sÄ±nÄ± inceler ve Ã¶rnek gÃ¶sterir."""

        if self.dataset is None:
            print("Ã–nce veri setini yÃ¼klemelisiniz!")
            return

        print("\n" + "=" * 60)
        print("VERÄ° SETÄ° YAPISI")
        print("=" * 60)

        # Ä°lk kaydÄ± gÃ¶ster
        first_item = self.dataset['train'][0]

        print("\nÄ°lk kayÄ±t Ã¶rneÄŸi:")
        print("-" * 60)
        for key, value in first_item.items():
            # Uzun metinleri kÄ±salt
            if isinstance(value, str) and len(value) > 200:
                print(f"{key}: {value[:200]}...")
            else:
                print(f"{key}: {value}")

        print("\nSÃ¼tun bilgileri:")
        print("-" * 60)
        for key in first_item.keys():
            print(f"â€¢ {key}")

        # BirkaÃ§ Ã¶rnek daha gÃ¶ster
        print("\nÃ–rnek kelimeler:")
        print("-" * 60)
        for i in range(min(10, len(self.dataset['train']))):
            item = self.dataset['train'][i]
            madde = item.get('madde', 'N/A')
            anlam = item.get('anlam', '')

            # Anlam tipini gÃ¶ster
            anlam_type = type(anlam).__name__
            print(f"{i + 1}. {madde} (anlam tipi: {anlam_type})")

            # Ä°lk anlamÄ± gÃ¶ster
            if isinstance(anlam, str) and anlam:
                try:
                    import json
                    anlam_parsed = json.loads(anlam)
                    if isinstance(anlam_parsed, list) and anlam_parsed:
                        first_anlam = anlam_parsed[0]
                        if isinstance(first_anlam, dict):
                            print(f"   â†’ {first_anlam.get('anlam', 'N/A')[:100]}")
                except:
                    print(f"   â†’ {anlam[:100]}")

    def process_data(self):
        """
        Veri setini RAG iÃ§in uygun formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.

        Bu veri setinde her satÄ±r tek bir kelime-anlam Ã§ifti iÃ§eriyor.
        Ã‡ok basit ve dÃ¼z bir yapÄ±.
        """

        if self.dataset is None:
            print("Ã–nce veri setini yÃ¼klemelisiniz!")
            return None

        print("\nVeri iÅŸleniyor...")

        self.processed_data = []
        error_count = 0

        # Her kelimeyi iÅŸle
        for item in tqdm(self.dataset['train'], desc="Ä°ÅŸleniyor"):
            try:
                # AlanlarÄ± al - None deÄŸerleri boÅŸ string'e Ã§evir
                kelime = str(item.get('madde', '')).strip()
                anlam = str(item.get('anlam', '')).strip()
                ornek = str(item.get('ornek', '') or '').strip()
                ai_ornek = str(item.get('ai_ornek', '') or '').strip()

                # Kelime veya anlam boÅŸ ise atla
                if not kelime or not anlam or kelime == 'None' or anlam == 'None':
                    error_count += 1
                    continue

                # Tam metin oluÅŸtur (RAG iÃ§in zengin context)
                full_text = f"Kelime: {kelime}\n\n"
                full_text += f"Anlam: {anlam}\n"

                # Ã–rnek varsa ekle
                if ornek and ornek != 'None':
                    full_text += f"\nÃ–rnek kullanÄ±m: {ornek}\n"

                # AI Ã¶rneÄŸi varsa ekle (daha detaylÄ±)
                if ai_ornek and ai_ornek != 'None':
                    full_text += f"\nDetaylÄ± Ã¶rnek: {ai_ornek}\n"

                # DokÃ¼man oluÅŸtur
                doc = {
                    'text': full_text.strip(),
                    'kelime': kelime,
                    'anlam': anlam,
                    'ornek': ornek if (ornek and ornek != 'None') else None,
                    'ai_ornek': ai_ornek if (ai_ornek and ai_ornek != 'None') else None
                }

                self.processed_data.append(doc)

            except Exception as e:
                error_count += 1
                continue

        print(f"{len(self.processed_data)} dokÃ¼man oluÅŸturuldu!")
        if error_count > 0:
            print(f"{error_count} kayÄ±t iÅŸlenirken hata oluÅŸtu (atlandÄ±)")

        return self.processed_data

    def save_processed_data(self, filename="processed_tdk.json"):
        """Ä°ÅŸlenmiÅŸ veriyi JSON olarak kaydeder."""

        if not self.processed_data:
            print("Ã–nce veriyi iÅŸlemelisiniz!")
            return

        filepath = os.path.join(self.cache_dir, filename)

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.processed_data, f, ensure_ascii=False, indent=2)

        print(f"Veri kaydedildi: {filepath}")

    def load_processed_data(self, filename="processed_tdk.json"):
        """Daha Ã¶nce kaydedilmiÅŸ iÅŸlenmiÅŸ veriyi yÃ¼kler."""

        filepath = os.path.join(self.cache_dir, filename)

        if not os.path.exists(filepath):
            print(f"Dosya bulunamadÄ±: {filepath}")
            return None

        with open(filepath, 'r', encoding='utf-8') as f:
            self.processed_data = json.load(f)

        print(f"{len(self.processed_data)} dokÃ¼man yÃ¼klendi!")
        return self.processed_data


# Test iÃ§in main fonksiyonu
if __name__ == "__main__":
    # Veri yÃ¼kleyiciyi oluÅŸtur
    loader = TDKDataLoader()

    # Veri setini yÃ¼kle
    loader.load_dataset()

    # Veri yapÄ±sÄ±nÄ± incele
    loader.explore_data()

    # Veriyi iÅŸle
    loader.process_data()

    # Ä°lk 3 dokÃ¼manÄ± gÃ¶ster
    if loader.processed_data:
        print("\n" + "=" * 60)
        print("Ä°LK 3 Ä°ÅLENMÄ°Å DOKÃœMAN")
        print("=" * 60)
        for i, doc in enumerate(loader.processed_data[:3], 1):
            print(f"\n{i}. DokÃ¼man:")
            print(f"Kelime: {doc['kelime']}")
            print(f"Text:\n{doc['text']}")

    # Ä°ÅŸlenmiÅŸ veriyi kaydet
    loader.save_processed_data()

    print("\nâœ¨ Ä°ÅŸlem tamamlandÄ±!")